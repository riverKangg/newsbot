# news_kakao_sender.py
#!/usr/bin/env python3

import os
import requests
from bs4 import BeautifulSoup
from datetime import datetime
from dotenv import load_dotenv
import openai
import json
import time
import pandas as pd

load_dotenv()

KAKAO_TOKEN = os.getenv("ACCESS_TOKEN")
REST_API_KEY = os.getenv("REST_API_KEY")
REFRESH_TOKEN = os.getenv("REFRESH_TOKEN")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

KAKAO_URL = "https://kapi.kakao.com/v2/api/talk/memo/default/send"
NAVER_NEWS_URL = "https://news.naver.com/"

HEADERS = {
    "Authorization": f"Bearer {KAKAO_TOKEN}",
    "Content-Type": "application/x-www-form-urlencoded"
}

openai.api_key = OPENAI_API_KEY



class NewsCrawler:
    def __init__(self, news_url, categories):
        self.news_url = news_url
        self.categories = categories

    def crawl(self):
        response = requests.get(self.news_url)
        soup = BeautifulSoup(response.text, "html.parser")
        news_items = []

        for link in soup.select("a"):
            title = link.get_text(strip=True)
            href = link.get("href")
            if not title or not href:
                continue

            for category, keywords in self.categories.items():
                for keyword in keywords:
                    if keyword in title:
                        content = self.get_article_content(href)
                        summary = self.summarize_with_gpt(content)
                        news_items.append({
                            "title": title,
                            "url": href,
                            "keyword": keyword,
                            "category": category,
                            "summary": summary
                        })
                        break

        return news_items

    def get_article_content(self, url):
        try:
            response = requests.get(url, timeout=5)
            soup = BeautifulSoup(response.text, "html.parser")

            content_div = soup.find("div", {"id": "newsct_article"})
            if not content_div:
                paragraphs = soup.find_all("p")
                return " ".join(p.get_text() for p in paragraphs)

            text = content_div.get_text(separator=" ", strip=True)
            return text.strip()
        except Exception as e:
            return "ë³¸ë¬¸ ë¶ˆëŸ¬ì˜¤ê¸° ì‹¤íŒ¨"

    def summarize_with_gpt(self, text):
        try:
            response = openai.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": "You are a helpful assistant that summarizes news articles."},
                    {"role": "user", "content": f"ë‹¤ìŒ ê¸°ì‚¬ì˜ ë‚´ìš©ì„ í•œì¤„ë¡œ ìš”ì•½í•´ì¤˜: {text}"}
                ],
                max_tokens=150,
                n=1,
                stop=None,
                temperature=0.5,
            )
            summary = response.choices[0].message.content.strip()
            return summary
        except Exception as e:
            print("GPT ìš”ì•½ ì˜¤ë¥˜:", e)
            return "ìš”ì•½ ì‹¤íŒ¨"




class NewsBot:
    def __init__(self):
        self.categories = {
            "ë‹¹ì‚¬": ["ì‚¼ì„±ìƒëª…", "í™ì›í•™"],
            "ë³´í—˜": ["ìƒëª…ë³´í—˜", "ì†í•´ë³´í—˜", "ìƒë³´", "ì†ë³´", "ë³´í—˜ì‚¬ê¸°", "ì‹¤ì†", "ë¬´í•´ì§€", "ì €í•´ì§€", "IFRS17", "í‚¥ìŠ¤",
"ì‚¼ì„±í™”ì¬", "í•œí™”ìƒëª…", "êµë³´ìƒëª…", "ì‹ í•œë¼ì´í”„"],
            "ê·¸ë£¹": ["ì´ì¬ìš©", "í™ë¼í¬", "ì´ë¶€ì§„", "ì´ì„œí˜„", "ì‚¼ì„±ì „ì", "ì‚¼ì„±ë¬¼ì‚°"],
            "ê¸ˆìœµ": ["ê¸ˆìœµìœ„", "ê¸ˆê°ì›", "ê¹€ë³‘í™˜", "ì´ë³µí˜„", "ê¸ˆìœµì§€ì£¼"]
        }

        self.news_crawler = NewsCrawler(news_url=NAVER_NEWS_URL, categories=self.categories)
        self.sent_articles = self.load_sent_articles()

    def load_sent_articles(self, filepath="sent_articles.json"):
        try:
            with open(filepath, 'r') as file:
                return set(json.load(file))
        except (FileNotFoundError, json.JSONDecodeError):
            return set()

    def save_sent_articles(self, filepath="sent_articles.json"):
        with open(filepath, 'w') as file:
            json.dump(list(self.sent_articles), file)

    def run(self):
        print("[ë‰´ìŠ¤ë´‡ ì‹¤í–‰ ì¤‘ â°]", datetime.now())
        news = self.news_crawler.crawl()

        if not news:
            print("ğŸ“­ ì „ì†¡í•  ë‰´ìŠ¤ ì—†ìŒ")
            return

        results = []

        for item in news:
            self.sent_articles.add(item['url'])
            print(f"ğŸ“¦ ê¸°ì‚¬ ì €ì¥: {item['title']}")
            results.append(item)

        self.save_sent_articles()

        # ì—‘ì…€ íŒŒì¼ ì €ì¥
        if results:
            df = pd.DataFrame(results)
            date_today = datetime.now().strftime("%Y%m%d")
            file_path = f"../data/daily/news_{date_today}.xlsx"
            df.to_excel(file_path, index=False)
            print(f"ğŸ—‚ ì—‘ì…€ íŒŒì¼ ì €ì¥: {file_path}")

if __name__ == "__main__":
    bot = NewsBot()
    bot.run()
