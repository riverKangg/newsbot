# news_list_scraper2.py

import os
import sys
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from webdriver_manager.chrome import ChromeDriverManager
from bs4 import BeautifulSoup
import pandas as pd
import time
from urllib.parse import quote

def web_driver():
    options = Options()
    options.add_argument('--headless=new')  # ìµœì‹  ë²„ì „ìš©!

    options.add_argument('--no-sandbox')
    options.add_argument('--disable-dev-shm-usage')
    options.add_argument('--disable-gpu')
    options.add_argument('--window-size=1920,1080')
    options.add_argument('--remote-debugging-port=9222')  # crash ë°©ì§€ìš©

    service = Service(ChromeDriverManager(driver_version="129.0.6668.59").install())

    driver = webdriver.Chrome(service=service, options=options)
    return driver

def build_naver_news_url(query, date):
    encoded_query = quote(query)
    option_date = f"ds={str(int(date[:4]))}.{date[4:6]}.{date[-2:]}&de={date[:4]}.{date[4:6]}.{date[-2:]}"
    url = f"https://search.naver.com/search.naver?where=news&query={encoded_query}&sm=tab_opt&sort=0&photo=0&field=0&pd=3&{option_date}"
    print(url)
    return url

def scroll_down(driver):
    last_height = driver.execute_script("return document.body.scrollHeight")
    while True:
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        time.sleep(2)
        new_height = driver.execute_script("return document.body.scrollHeight")
        if new_height == last_height:
            break
        last_height = new_height

def naver_news_scraper(query, date, category):
    url = build_naver_news_url(query, date)
    driver = web_driver()
    news_list = []
    try:
        driver.get(url)
        time.sleep(1)
        scroll_down(driver)
        soup = BeautifulSoup(driver.page_source, "html.parser")
        news_items = soup.select(".news_area")
        for item in news_items:
            title_elem = item.select_one(".news_tit")
            title = title_elem.text.strip()
            link = title_elem["href"]
            press_elem = item.select_one(".info.press")
            press = press_elem.text.strip() if press_elem else "ì–¸ë¡ ì‚¬ ì •ë³´ ì—†ìŒ"
            press = press.replace("ì–¸ë¡ ì‚¬ ì„ ì •","")
            desc_elem = item.select_one(".dsc_txt_wrap")
            description = desc_elem.text.strip() if desc_elem else "ìš”ì•½ ì •ë³´ ì—†ìŒ"

            test = item.find("div", class_="info_group")
            naver_link = test.find_all('a')[-1].get('href') if 'naver' in test.find_all('a')[-1].get('href') else None

            news_list.append({
                "ì œëª©": title,
                "ì–¸ë¡ ì‚¬": press,
                # "ìš”ì•½": description,
                "ë§í¬": link,
                "ë„¤ì´ë²„ë§í¬": naver_link,
                "ë¶„ë¥˜": category,
                "í‚¤ì›Œë“œ": query  # ê° ë‰´ìŠ¤ í•­ëª©ì— í•´ë‹¹ í‚¤ì›Œë“œ ì¶”ê°€
            })
    finally:
        driver.quit()

    return news_list

def save_to_excel(file_prefix, all_news_list, date):
    if all_news_list:
        data_directory = "../data/"
        if not os.path.exists(data_directory):
            os.makedirs(data_directory)
        df = pd.DataFrame(all_news_list)
        df = df[~df['ì œëª©'].str.contains('ìš´ì„¸')]
        if file_prefix=='health':
            desired_press = ['ì¡°ì„ ì¼ë³´', 'ì¤‘ì•™ì¼ë³´','ë™ì•„ì¼ë³´','í•œêµ­ê²½ì œ','ë§¤ì¼ê²½ì œ']
            df = df[df['ì–¸ë¡ ì‚¬'].isin(desired_press)]
        filename = f"{data_directory}{file_prefix}_{date}.xlsx"
        df.to_excel(filename, index=False)
        print(f"ë‰´ìŠ¤ ë°ì´í„°ê°€ {filename} íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    print("\nğŸ“„ ì‚¬ìš©ë²•: python news_list_scraper.py [health|cnews] [ë‚ ì§œ: YYYYMMDD]")

    if len(sys.argv) != 3:
        print("\nâ— ì¸ì ì˜¤ë¥˜: íŒŒì¼ ì ‘ë‘ì‚¬ì™€ ë‚ ì§œë¥¼ ì •í™•íˆ ì…ë ¥í•´ ì£¼ì„¸ìš”.")
        sys.exit(1) 

    file_prefix = sys.argv[1]
    date_str = sys.argv[2]

    if file_prefix=='health':
        selected_keywords = {
            "ì§ˆë³‘/ê±´ê°•": [
                "ë‹¹ë‡¨", "ê³ í˜ˆì••", "ì¹˜ë§¤", "ì•”", "ì‹¬ì¥", "ë‡Œì¡¸ì¤‘",
                "ê°‘ìƒì„ ", "ìœ„ì•”", "ìœ ë°©ì•”", "ëŒ€ìƒí¬ì§„",
                "ê³¨ë‹¤ê³µì¦", "ê°„ê²½í™”", "íë ´"
            ],
            "ì˜ˆë°©/ìƒí™œìŠµê´€": ["ê±´ê°•ê²€ì§„", "ì˜ˆë°©ì ‘ì¢…", "ìš´ë™", "ì‹ìŠµê´€",
                "ê¸ˆì—°", "ì ˆì£¼", "ìƒí™œìŠµê´€ë³‘", "ìŠ¤íŠ¸ë ˆìŠ¤", "ìˆ˜ë©´"],
            "ë…¸ë ¹í™”/ê³ ë ¹ì¸µ": ["ë…¸ì¸", "ê³ ë ¹ì", "ë…¸í›„", "ì‹œë‹ˆì–´", "60ëŒ€", "70ëŒ€", "ìš”ì–‘ë³‘ì›",
"ì¥ê¸°ìš”ì–‘", "ê°„ë³‘"]
        }
    elif file_prefix=='cnews':
        selected_keywords = {
            "ë‹¹ì‚¬": ["ì‚¼ì„±ìƒëª…", "í™ì›í•™"],
            "ë³´í—˜": ["ìƒëª…ë³´í—˜", "ì†í•´ë³´í—˜", "ìƒë³´", "ì†ë³´", "ë³´í—˜ì‚¬ê¸°",
                    "ì‹¤ì†", "ë¬´í•´ì§€", "ì €í•´ì§€", "IFRS17", "í‚¥ìŠ¤",
                    "ì‚¼ì„±í™”ì¬", "í•œí™”ìƒëª…", "êµë³´ìƒëª…", "ì‹ í•œë¼ì´í”„"],
            "ê·¸ë£¹": ["ì´ì¬ìš©", "í™ë¼í¬", "ì´ë¶€ì§„", "ì´ì„œí˜„", "ì‚¼ì„±ì „ì", "ì‚¼ì„±ë¬¼ì‚°"],
            "ê¸ˆìœµ": ["ê¸ˆìœµìœ„", "ê¸ˆê°ì›", "ê¹€ë³‘í™˜", "ì´ë³µí˜„", "ê¸ˆìœµì§€ì£¼"]
        }

    all_news_list = []

    for category, keyword_list in selected_keywords.items():
        for query in keyword_list:
            news = naver_news_scraper(query, date_str, category)
            all_news_list.extend(news)

    save_to_excel(file_prefix, all_news_list, date_str)
